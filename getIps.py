#!/usr/bin/env python
#-*- coding:utf-8 -*-
#@Time:2020/6/1323:52
#@Author:mery
#@File:getIps.py


import requests
from bs4 import BeautifulSoup
from lxml import etree
import subprocess as sp
import random
import re


"""
函数说明:获取代理ip网站的ip
"""
def get_proxys(page):
    #requests的Session()可以自动保存cookie，
    #不需要自己维护cookie内容
    S = requests.Session()
    #目标网址的url
    target_url = 'http://www.xicidaili.com/nn/%d' %page
    target_headers = {
        'Upgrade-Insecure-Requests': '1',
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Referer': 'http://www.xicidaili.com/nn/',
        'Accept-Encoding': 'gzip, deflate, sdch',
        'Accept-Language': 'zh-CN,zh;q=0.8'
    }
    target_response = S.get(url=target_url,
                            headers=target_headers)
    target_response.encoding = 'utf-8'
    target_html = target_response.text
    # print(target_html)

    #解析数据（ip,port,protocol）
    bf1_ip_list = BeautifulSoup(target_html,'lxml')
    bf2_ip_list = BeautifulSoup(str(bf1_ip_list.find_all(id='ip_list')),'lxml')
    ip_list_info = bf2_ip_list.table.contents

    proxys_list = []
    for index in range(len(ip_list_info)):
        if index % 2 == 1 and index != 1:
            dom = etree.HTML(str(ip_list_info[index]))
            ip = dom.xpath('//td[2]')
            port = dom.xpath('//td[3]')
            protocol = dom.xpath('//td[6]')
            proxys_list.append(protocol[0].text.lower()
                               + "#" + ip[0].text
                               + "#" + port[0].text)
    print(proxys_list)
    return proxys_list

"""
函数说明:检测代理ip的连通性
参数:
    ip--代理的ip地址
    lose_time--匹配的丢包数
    waste_time--匹配平均时间
返回值:
    average_time--代理ip的平均耗时
"""
def check_ip(ip, lose_time, waste_time):
    cmd = "ping -n 3 -w 3 %s"
    #执行命令
    p = sp.Popen(cmd %ip, stdin=sp.PIPE,
                 stdout=sp.PIPE,
                 stderr=sp.PIPE,
                 shell=True)
    #获取返回结果并解码
    out = p.stdout.read().decode('GBK')
    lose_time = lose_time.findall(out)

    if len(lose_time) == 0:
        lose = 3
    else:
        lose = int(lose_time[0])
    #如果丢包数大于2，那么我们返回平均耗时1000
    if lose > 2:
        #返回false（1000）
        return 1000
    else:
        #平均时间
        average = waste_time.findall(out)
        if len(average) == 0:
            return 1000
        else:
            average_time = int(average[0])
            #返回平均耗时
            return average_time


"""
函数说明:初始化正则表达式
返回值:
    lose_time--匹配丢包数
    waste_time--匹配平均时间
"""
def initpattern():
    #匹配丢包数
    lose_time = re.compile(u"丢失 = (\d+)",re.IGNORECASE)
    #匹配平均时间
    waste_time = re.compile(u"平均 = (\d+)ms",re.IGNORECASE)
    return lose_time, waste_time

def userIp():
    #初始化正则表达式
    lose_time, waste_time = initpattern()
    proxys_list = ['http#111.222.141.127#8118', 'http#125.126.120.103#60004', 'http#125.126.98.79#60004', 'https#119.254.94.114#45691', 'https#119.133.85.221#4216', 'https#223.247.92.78#4216', 'https#114.98.25.197#4216', 'https#119.142.242.49#4216', 'https#36.6.224.92#3000', 'http#60.167.134.176#808', 'https#218.59.193.14#47138', 'http#223.241.1.128#3000', 'http#114.234.166.26#8118', 'http#58.246.143.32#8118', 'http#223.214.221.133#3000', 'https#211.152.33.24#48749', 'http#222.190.125.2#8118', 'http#222.85.28.130#40505', 'https#223.247.92.128#4216', 'https#117.71.169.65#3000', 'http#124.90.53.44#8888', 'https#115.219.106.91#3128', 'http#125.126.121.66#60004', 'https#116.27.245.228#4216', 'https#125.92.102.4#4216', 'https#121.233.86.229#4216', 'http#106.14.76.134#8080', 'http#222.186.55.41#8080', 'https#221.224.136.211#35101', 'http#117.71.166.44#3000', 'http#121.233.87.151#4216', 'http#218.58.193.98#8060', 'http#223.241.2.207#4216', 'https#223.247.95.226#4216', 'https#223.241.0.68#4216', 'http#183.64.239.19#8060', 'http#223.167.15.98#8060', 'http#125.126.115.171#60004', 'http#125.126.106.48#60004', 'https#125.126.107.10#60004', 'https#117.71.166.17#3000', 'http#60.189.207.80#60004', 'https#113.128.148.31#8118', 'https#223.241.7.244#4216', 'https#183.4.64.82#4216', 'https#113.101.158.79#4216', 'http#114.104.138.87#3000', 'https#223.247.92.157#4216', 'http#60.2.44.182#30963', 'https#58.220.95.114#10053', 'https#117.71.169.157#3000', 'https#58.220.95.116#10122', 'https#119.254.94.93#46323', 'http#117.71.171.78#3000', 'https#111.231.239.143#1081', 'http#120.198.76.45#41443', 'http#113.128.148.50#8118', 'https#175.148.79.101#1133', 'http#114.220.29.95#3128', 'http#222.190.217.156#8118', 'http#61.135.155.82#443', 'https#124.93.201.59#59618', 'https#223.247.94.22#4216', 'https#223.247.95.168#4216', 'https#36.59.110.103#4216', 'https#116.113.27.170#47849', 'http#223.241.4.184#4216', 'http#36.59.117.170#4216', 'https#36.59.111.84#4216', 'http#59.44.78.30#42335', 'http#119.179.128.143#8060', 'https#49.74.20.118#8118', 'http#211.149.128.21#8080', 'http#183.162.167.155#4216', 'http#49.89.143.39#3000', 'http#114.98.26.84#4216', 'http#27.42.168.46#48919', 'http#36.59.120.206#4216', 'http#27.43.87.50#8118', 'http#113.103.226.99#4216', 'https#223.241.2.11#4216', 'http#27.188.62.3#8060', 'https#221.227.111.146#808', 'https#58.61.154.153#8080', 'https#61.145.34.174#4216', 'https#61.160.245.88#51332', 'http#139.196.52.1#8080', 'http#117.71.164.232#3000', 'https#36.59.121.144#4216', 'https#223.241.2.50#4216', 'https#61.145.34.71#4216', 'https#36.59.120.61#4216', 'https#36.59.117.248#4216', 'http#114.103.19.147#3000', 'https#36.59.110.114#4216', 'https#223.241.2.226#4216', 'http#223.243.4.57#4216', 'http#175.148.76.161#1133', 'https#119.130.165.54#4216', 'http#139.196.48.252#8080']
     #如果平均时间超过200ms，则重新选取ip
    while True:
        #从100个ip中随机选取一个ip作为代理进行网络访问
        proxy = random.choice(proxys_list)
        split_proxy = proxy.split('#')
        #获取ip
        ip = split_proxy[1]
        #检查ip
        average_time = check_ip(ip, lose_time, waste_time)

        if average_time > 200:
            #去掉不能使用的ip
            proxys_list.remove(proxy)
            print("ip链接超时，重新获取中...")
        else:
            break
    proxys_list.remove(proxy)
    '''
    proxys_dict = {split_proxy[0]:split_proxy[1]
                    + ":" + split_proxy[2]}
    print(proxys_dict)
    '''
    proxy = {'http':split_proxy[0]+'://'+ split_proxy[1]+':'+split_proxy[2],
             'https':split_proxy[0]+'://'+ split_proxy[1]+':'+split_proxy[2]}
    print(proxy)
    return proxy



if __name__ == '__main__':

    response = requests.get("https://www.baidu.com",proxies=userIp())
    print(response)

    '''
    #初始化正则表达式
    lose_time, waste_time = initpattern()

    #获取ip代理
    proxys_list = get_proxys(1)


    #如果平均时间超过200ms，则重新选取ip
    while True:
        #从100个ip中随机选取一个ip作为代理进行网络访问
        proxy = random.choice(proxys_list)
        split_proxy = proxy.split('#')
        #获取ip
        ip = split_proxy[1]
        #检查ip
        average_time = check_ip(ip, lose_time, waste_time)

        if average_time > 200:
            #去掉不能使用的ip
            proxys_list.remove(proxy)
            print("ip链接超时，重新获取中...")
        else:
            break

    proxys_list.remove(proxy)


    proxys_dict = {split_proxy[0]:split_proxy[1]
                    + ":" + split_proxy[2]}
    print("使用代理:", proxys_dict)
    '''

